{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](images/calc.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l1 = 0.05\n",
    "l2 = 0.10\n",
    "o1 = 0.90\n",
    "o2 = 0.78"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A single Neuron\n",
    "inputs = [1.2,5.6,2.1]\n",
    "weights = [3.1,2.1,9.8]\n",
    "bias = 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = inputs[0]*weights[0] + inputs[1]*weights[1] + inputs[2]*weights[2] + bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "37.06"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A single Layer\n",
    "inputs = [1.2,5.6,2.1,2.5]\n",
    "weights1 = [3.1,2.1,9.8,1.0]\n",
    "weights2 = [5.1,6.1,4.8,7.0]\n",
    "weights3 = [6.1,4.1,5.8,6.0]\n",
    "bias1 = 1.0\n",
    "bias2 = 2.0\n",
    "bias3 = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = [inputs[0]*weights1[0] + inputs[1]*weights1[1] + inputs[2]*weights1[2] + inputs[3]* weights1[3] + bias1,\n",
    "         inputs[0]*weights2[0] + inputs[1]*weights2[1] + inputs[2]*weights2[2] + inputs[3]* weights2[3] + bias2,\n",
    "         inputs[0]*weights3[0] + inputs[1]*weights3[1] + inputs[2]*weights3[2] + inputs[3]* weights3[3] + bias3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = [1.2,5.6,2.1,2.5]\n",
    "weights = [[3.1,2.1,9.8,1.0],\n",
    "          [5.1,6.1,4.8,7.0],\n",
    "          [6.1,4.1,5.8,6.0]]\n",
    "\n",
    "bias = [1.0,2.0,0.5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = np.dot(weights,inputs) + bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_bias_and_weights(inputs,neurons):\n",
    "    weights = 0.10*(np.random.randn(inputs,neurons))\n",
    "    biases = np.zeros((1,neurons))\n",
    "    print(weights)\n",
    "    print(biases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.87031738e-05  3.76104327e-02 -4.79964613e-02 ...  4.33146550e-02\n",
      "   6.30130543e-02 -7.58168838e-02]\n",
      " [ 9.05332548e-02  1.06441101e-01  1.26596001e-01 ... -1.06334279e-02\n",
      "  -1.29686207e-01 -1.00887956e-01]\n",
      " [ 5.51779071e-02 -3.39386780e-03  5.78234865e-02 ... -5.58337276e-02\n",
      "  -1.38230017e-01 -1.75694942e-01]\n",
      " [-7.90398194e-02 -9.27575206e-03 -2.69579636e-02 ... -1.57268677e-01\n",
      "  -6.81319666e-02 -1.53889803e-02]]\n",
      "[[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "create_bias_and_weights(4,512)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's first understand how backpropagation works\n",
    "\n",
    "![alt text](https://i.ytimg.com/vi/An5z8lR8asY/maxresdefault.jpg \"Logo Title Text 1\")\n",
    "\n",
    "In 1986 Hinton released [this](http://www.cs.toronto.edu/~hinton/absps/naturebp.pdf) paper detailing a new optimization strategy for neural networks called 'backpropagation'. This paper is the reason the current Deep Learning boom is possible. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3 concepts behind Backpropagtion (From Calculus)\n",
    "\n",
    "1. Derivative\n",
    "![alt text](https://i.imgur.com/eRF9pXu.jpg \"Logo Title Text 1\")\n",
    "\n",
    "2. Partial Derivative\n",
    "\n",
    "![alt text](https://i.imgur.com/Rergqbt.jpg \"Logo Title Text 1\")\n",
    "\n",
    "3. Chain Rule\n",
    "\n",
    "![alt text](https://i.imgur.com/HFmGQyH.jpg \"Logo Title Text 1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How do artificial & biological neural nets compare?\n",
    "\n",
    "Artificial Neural Networks are inspired by the hierarchial structure of brains neural network\n",
    "\n",
    "![alt text](https://appliedgo.net/media/perceptron/neuron.png \"Logo Title Text 1\")\n",
    "\n",
    "The brain has \n",
    "-100 billion neurons \n",
    "-- Each neuron has\n",
    "   - A cell body w/ connections\n",
    "   - numerous dendrites \n",
    "   - A single axon \n",
    "- Parallel chaining (each neurons connected to 10,000+ others)\n",
    "- Great at connecting different concepts\n",
    "\n",
    "Computers have\n",
    "- Not neurons, but transistors made in silicon!\n",
    "- Serially chained (each connected to 2-3 others (logic gates))\n",
    "- Great at storage and recall\n",
    "\n",
    "Some key differences\n",
    "- All sensory or motor systems in the brain are recurrent\n",
    "- Sensory systems tend to have lots of lateral inhibition (neurons inhibiting other neurons in the same layer)\n",
    "- There is no such thing as a fully connected layer in the brain, connectivity is usually sparse (though not random).\n",
    "- brains are born pre-wired to learn without supervision.\n",
    "- The Brain is low power. Alpha GO consumed the power of 1202 CPUs and 176 GPUs, not to train, but just to run. Brainâ€™s power consumption is ~20W.\n",
    "\n",
    "![alt text](https://images.gr-assets.com/books/1348246481l/5080355.jpg\n",
    " \"Logo Title Text 1\")\n",
    "\n",
    "\"the brain is not a blank slate of neuronal layers \n",
    "waiting to be pieced together and wired-up; \n",
    "we are born with brains already structured \n",
    "for unsupervised learning in a dozen cognitive \n",
    "domains, some of which already work pretty well \n",
    "without any learning at all.\" - Steven Pinker\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "X =  [[3.1,2.1,9.8,1.0],\n",
    "          [5.1,6.1,4.8,7.0],\n",
    "          [6.1,4.1,5.8,6.0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dense():\n",
    "    def __init__(self,inputs,neurons):\n",
    "        self.weights = 0.10*np.random.randn(inputs,neurons)\n",
    "        self.biases = np.zeros((1,neurons))\n",
    "    def forward(self,inputs):\n",
    "        self.output = np.dot(inputs,self.weights) + self.biases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer1 = Dense(4,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.Dense at 0x25b59f55588>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer1.forward(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-1.59763115  1.15761941  0.96805349  1.6001348  -0.24907378]\n",
      " [-0.47178883  1.76529298 -0.42911209  1.14562052  0.02536798]\n",
      " [-0.37369306  1.82889211 -0.02105575  1.25511121  0.2757226 ]]\n"
     ]
    }
   ],
   "source": [
    "print(layer1.output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer2 = Dense(5,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer2.forward(layer1.output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.15723252  0.07958189]\n",
      " [-0.18249872 -0.20111198]\n",
      " [-0.03899815 -0.17700986]]\n"
     ]
    }
   ],
   "source": [
    "print(layer2.output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Relu():\n",
    "    def forward(self,inputs):\n",
    "        self.output = np.maximum(0,inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "activation = Relu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "activation.forward(layer1.output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.         1.15761941 0.96805349 1.6001348  0.        ]\n",
      " [0.         1.76529298 0.         1.14562052 0.02536798]\n",
      " [0.         1.82889211 0.         1.25511121 0.2757226 ]]\n"
     ]
    }
   ],
   "source": [
    "print(activation.output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
